<!-- //about me  -->
<html lang="en">
    
    <head>
      <meta charset="utf-8">
      <title>KL</title>
      <link rel="stylesheet" href="layout/style.css">
      <link href="https://fonts.googleapis.com/css?family=Geo" rel="stylesheet">
      <link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono" rel="stylesheet">
      <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:300" rel="stylesheet">
    </head> 

    <body>

      <div class='navi'>
          <p class='header'><a href="index.html"><strong>KiniLuo</strong> ://</a></p>
          <br>
          <p class='class'>./<a href="dataViz.html">Data Visualization</a></p>
              <p class='project'>
                <a href="dataDashboard.html">TVL Data Quality Dashboard</a>
                <br>
                <a href="fakeNews.html"><u>Tracking the Twitter Bots</u></a>
                <br>
                <a href="dataDiary.html">Data Diary With ChatBots</a>
                <br>
                <a href="googleTrends.html">Google Trends - R/GA Signage</a>
                <br>
                <a href="eyeTracker.html">VR Eye Tracker</a>
                <br>
                <a href="colorTaxonomy.html">Color Palette of Cities</a>
                <br>
                <a href="birthWeight.html">The Gaps of Birth Weight</a>
              </p>
        
      
          <br>
          <p class='class'>./<a href="info.html">InfoGraphics</a></p>
              <p class='project'>
                <a href="collection.html">A Collection of Infographics</a>
                <br>
                <a href="conceptMap.html">Concept Maps for Design Thinking</a>
              </p>
        

          <br>
          <p class='class'>./<a href="graphic.html">Graphic Design + Typography</a></p>
              <p class='project'>
                <a href="borobudure.html">Identity Design for Borobudur </a>
                <br>
               <!--  <a href="grillSpot.html">Grill Spot</a>
                <br> -->
                <a href="typography.html">Typography Practices</a>
              </p>
          

          <br>    
          <p class='class'>./<a href="photo.html">Photography + Videography</a></p>
              <p class='project'>
                <a href="onTheWay.html">On the Way</a>
                <br>
                <a href="shift.html">Samsung Shift</a>
              </p>
          
        <!--  <br>  
         <p class='class'>./<a>UX + UI</a></p>
              <p class='project'>
                <a href="homeless.html">Designing for Homeless</a>
                <br>
                <a href="apiSite.html">TruValue Labs API Site</a>
              </p> -->

          <br>
          <p class='class'>./<a href="nothing.html">Sometimes <br>Making Sth Leads to Nothing
          </a></p>

          <br>
          <p class='class'>./<a href="about.html">About</a></p>
           
      </div>



    <div class='content'>

        <p class="title"><strong>Tracking the Twitter Bots</strong></p>
        <br>
        <p id="time">05.2017 _ now  [ a work in progress ]</p>
        <p class="content_text">
         <img class="content_img_group1" src="img/fakenews/head.png" alt="mock1" width=100% > <br><br><br>
          People are strange when you are stranger, especially encountering online.
          Maybe this person is even not a person, neither a bot.<br><br>

            The impulse for creating this project include but not only include the curiosities listed below:
            <br><br>
            How can a seemingly human twitter user metamorphose to a twitter bot? <br><br>
            Are there any criteria we could use to make a reasonable judgment on the reliability of the information we ingested from social media on a daily basis? <br><br>
            WTF is going on on the social media... ...

          <br><br><br><br>
            <strong>1.About The Data</strong>
<br><br>
The data scientists in the Lazer Lab from Northeastern University created an algorithm to cluster 7 predominant sites that tweeter users most frequently retweeted links from through Aug 2016 to Feb 2017, during presidential election.<br>
Besides 6 news sites that have certain levels of credibility from the public, there is also a special group that is labeled as "FAKE NEWS" which is a medley of 449 sites that are defined as fake news sites.
<br>
The data scientists then identified individuals with unique names and locations from the voter records, and match them with Twitter users who share the same profile. 
<br>
The final step is that the data scientists identified the most active top 20 sharers for each of these 7 sites, as well as their activities on Twitter during the election. 
<br>
Thus, here we get a simple CSV file which annotates the "source and target" relationship of this super-sharer network<br>
AND<br>
A considerably large set of JSON file to describe the behavior of each of these active super-sharers.
          <br><br><br><br>
            <strong>2.Initial Exploration</strong>
<br><br>
Below is a static data visualization that presents comparisons amount twitter accounts that are suspicious at the initial stage.
            <br><br>
           <img class="content_img_group1" src="img/fakenews/fakenews.png" alt="mock1" width=100% > <br><br>
One could roughly conclude from this infographic that traditional news agencies like New York Times and CNN locate more distant from the center of this network rather than social media such as FACEBOOK and YOUTUBE.
<br>
If one looks at how different users inside this network tweets over the election, it's not hard to spot that there is a huge difference in the daily aggregated volume and frequency one tweets by looking at the area-charts and heat-maps. The three significant electorial events mapped on top of the time-series implies certain users are ultra-active on these dates.
            <br><br><br><br>
            <strong>3.One Step Furthur While Making it Interactive <br>(This is where I am currently at)</strong>
<br><br>
The static visualization gives a finite summary of how the data could look like but the information it reals is far less than what is really in there. Thus, making it interactive and exploratory is a solution. 
<br>
Following are some modular prototypes I created to test the performance of my code and saved for later using one the data pipeline is ready.
<br><br>
          [UPDATE THE INTERNAL FORCE]    
          <iframe src="https://player.vimeo.com/video/271589225" width=100% height="605" frameborder="0"  controls autoplay loop=infinite></iframe>
           <br><br><br><br>
          [TOGGLE BETWEEN HEATMAP]
            <iframe src="https://player.vimeo.com/video/271597885" width=100% height="203" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
            <br><br><br><br>
          [VOLUME INTERACTION]
            <br><br><br><br>
            <iframe src="https://player.vimeo.com/video/271598168" width="640" height="354" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
            
            
          <br><br><br>
          The bi-product of this fun experiment is my MFA Thesis book which talks a lot about Embodied Interaction and how to design for Conversational User Interface.
          Not super mature writing but you could still take a look at 
          <u><a href="img/datadiary/KiniLuo-MFAthesis.pdf" target="_blank">MY MFA THESIS</a></u> since there are plenty of concept maps I created for CUI and some discussion about how to deal with bot as human.

          <br><br>
           <br><br><br><br>
           _tools:<br>
           d3.js,jQuery,ScrollMagic,React,node.js,mongoDB,IBM Watson,adobe creative suite



        </p>
        <br><br><br><br>
        <div class="footer">Kini Luo / 2015_2017</div>
          
    </div>

        
    
   
    </body>

  <script src="module/d3.min.js"></script>
  <script src="module/jquery-3.0.0.min.js"></script>
  <script src="module/three.min.js"></script>
  <script src="module/TweenMax.min.js" ></script>
  <script src="script/script.js"></script>
    
</html>